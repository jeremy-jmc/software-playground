@article{ALMEIDA2024101677,
title = {AICodeReview: Advancing code quality with AI-enhanced reviews},
journal = {SoftwareX},
volume = {26},
pages = {101677},
year = {2024},
issn = {2352-7110},
doi = {https://doi.org/10.1016/j.softx.2024.101677},
url = {https://www.sciencedirect.com/science/article/pii/S2352711024000487},
author = {Yonatha Almeida and Danyllo Albuquerque and Emanuel Dantas Filho and Felipe Muniz and Katyusco {de Farias Santos} and Mirko Perkusich and Hyggo Almeida and Angelo Perkusich},
keywords = {Artificial Intelligence (AI), Code review, Automated assessment, Software quality enhancement},
abstract = {This paper presents a research investigation into the application of Artificial Intelligence (AI) within code review processes, aiming to enhance the quality and efficiency of this critical activity. An IntelliJ IDEA plugin was developed to achieve this objective, leveraging GPT-3.5 as the foundational framework for automated code assessment. The tool comprehensively analyses code snippets to pinpoint syntax and semantic issues while proposing potential resolutions. The study showcases the tool's architecture, configuration methods, and diverse usage scenarios, emphasizing its effectiveness in identifying logic discrepancies and syntactical errors. Finally, the findings suggest that integrating AI-based techniques is a promising approach to streamlining the time and effort invested in code reviews, fostering advancements in overall software quality.}
}

@misc{Oliveira_Rios_Jiang_2023, 
title={AI-powered peer review process},
url={http://dx.doi.org/10.14742/apubs.2023.482},
doi={10.14742/apubs.2023.482},
journal={ASCILITE Publications},
publisher={Australasian Society for Computers in Learning in Tertiary Education},
author={Oliveira, Eduardo and Rios, Shannon and Jiang, Zhuoxuan},
year={2023}, 
month=nov,
pages={184-194},
abstract={Code review is a common type of peer review in Computer Science (CS) education. It's a peer review process that involves CS students other than the original author examining source code and is widely acknowledged as an effective method for reducing software errors and enhancing the overall quality of software projects. While code review is an essential skill for CS students, they often feel uncomfortable to share their work or to provide feedback to peers due to concerns related to coding experience, validity, reliability, bias, and fairness. An automated code review process could offer students the potential to access timely, consistent, and independent feedback about their coding artifacts. We investigated the use of generative Artificial Intelligence (genAI) to automate a peer review process to enhance CS students' engagement with code review in an industry-based subject in the School of Computing and Information System, University of Melbourne. Moreover, we evaluated the effectiveness of genAI at performing checklist-based assessments of code. A total of 80 CS students performed over 36 reviews in two different weeks. We found our genAI-powered reviewing process significantly increased students' engagement in code review and, could also identify a larger number of code issues in short times, leading to more fixes. These results suggest that our approach could be successfully used in code reviews, potentially helping to address issues related to peer review in higher education settings.}
}


@article{Rasheed_etal, 
title={AI-powered Code Review with LLMs: Early Results}, url={https://arxiv.org/abs/2404.18496},
doi={10.48550/ARXIV.2404.18496}, abstract={In this paper, we present a novel approach to improving software quality and efficiency through a Large Language Model (LLM)-based model designed to review code and identify potential issues. Our proposed LLM-based AI agent model is trained on large code repositories. This training includes code reviews, bug reports, and documentation of best practices. It aims to detect code smells, identify potential bugs, provide suggestions for improvement, and optimize the code. Unlike traditional static code analysis tools, our LLM-based AI agent has the ability to predict future potential risks in the code. This supports a dual goal of improving code quality and enhancing developer education by encouraging a deeper understanding of best practices and efficient coding techniques. Furthermore, we explore the model's effectiveness in suggesting improvements that significantly reduce post-release bugs and enhance code review processes, as evidenced by an analysis of developer sentiment toward LLM feedback. For future work, we aim to assess the accuracy and efficiency of LLM-generated documentation updates in comparison to manual methods. This will involve an empirical study focusing on manually conducted code reviews to identify code smells and bugs, alongside an evaluation of best practice documentation, augmented by insights from developer discussions and code reviews. Our goal is to not only refine the accuracy of our LLM-based tool but also to underscore its potential in streamlining the software development lifecycle through proactive code improvement and education.},
publisher={arXiv},
author={Rasheed, Zeeshan and Sami, Malik Abdul and Waseem, Muhammad and Kemell, Kai-Kristian and Wang, Xiaofeng and Nguyen, Anh and Systä, Kari and Abrahamsson, Pekka},
year={2024}
}

@inproceedings{Google_etal,
title={Resolving Code Review Comments with Machine Learning},
author={Alexander Frömmgen and Jacob Austin and Peter Choy and Nimesh Ghelani and Lera Kharatyan and Gabriela Surita and Elena Khrapko and Pascal Lamblin and Pierre-Antoine Manzagol and Marcus Revaj and Maxim Tabachnyk and Daniel Tarlow and Kevin Villela and Dan Zheng and Satish Chandra and Petros Maniatis},
year={2024},
booktitle={2024 IEEE/ACM 46th International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP)}
}